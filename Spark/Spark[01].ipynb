{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 입문\n",
    "\n",
    "### 사전 작업\n",
    "java가 깔려있어야 한다.\n",
    "java 있는지 확인(C>Program Files>Java 폴더 확인) 후 anaconda 관리자 권한 실행\n",
    "conda install -c conda-forge pyspark (오래 걸릴 수 있다. done까지 완료)\n",
    "\n",
    "## 요약\n",
    "RDD의 개념을 익히고 Pair RDD, 외부 데이터를 로드해서 RDD 생성, Spark DataFrame을 생성해봤다.\n",
    "\n",
    "## RDD의 개념\n",
    "RDD : Resilient Disributed Data\n",
    "\n",
    "- RDD는 두 가지의 연산으로 이루어져 있다.\n",
    "- Transformation, Action에 대한 이해가 필요\n",
    "\n",
    "- Transformation -> Lazy Execution 또는 Lazy Loading\n",
    "- 트랜스포메이션이 행해지면, RDD가 수행되는 것이 아니라, 새로운 RDD를 만들어 내고 그 새로운 RDD에 수행결과를 저장한다. (commit, push 같은 개념)\n",
    "\n",
    "\n",
    "## RDD의 Operation\n",
    "### Transformation\n",
    "기존의 RDD 데이타를 변경하여 새로운 RDD 데이타를 생성해내는 것. 흔한 케이스는 filter와 같이 특정 데이타만 뽑아 내거나 map 함수 처럼, 데이타를 분산 배치 하는 것 등을 들 수 있다.\n",
    "\n",
    "### Action\n",
    "Method로 이루어진 실행 작업이며, Transformation이 행해지고 나서 이루어지는 Evaluation 작업\n",
    "RDD 값을 기반으로 무엇인가를 계산해서(computation) 결과를 (셋이 아닌) 생성해 내는것으로 가장 쉬운 예로는 count()와 같은 operation들을 들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-CS5B0E0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=sparkApp>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf  = SparkConf().setMaster('local').setAppName('sparkApp')\n",
    "spark = SparkContext(conf=conf)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./data/test.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.textFile('./data/test.txt')\n",
    "rdd\n",
    "# Lazy Loading\n",
    "# 없는 파일을 불러오는데도  Error가 나지 않는다. RDD가 만들어졌을 뿐 실행되진 않은 것이기 때문.\n",
    "# Transformation을 한 것. RDD가 수행된 것이 아니라 저장된 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = rdd.filter(lambda x : 'spark' in x)\n",
    "lines\n",
    "# filter를 사용한 것이 Action이다. 결과는 Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 생성\n",
    "- 데이터를 직접 만드는 방법( parallelize() ), 외부 데이터를 로드 방법으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd = spark.parallelize(['test', 'this is a test rdd'])\n",
    "sample_rdd\n",
    "# RDD 라는 변수의 타입을 만드는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'this is a test rdd']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd.collect()\n",
    "# 연산을 수행하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 에서 자주 쓰는 연산 함수\n",
    "- collect() : RDD에 Transfomation 된 결과를 return하는 함수\n",
    "- map() : 연산을 수행할 때 사용하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = spark.parallelize(list(range(5)))\n",
    "numbers\n",
    "# RDD 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = numbers.map(lambda x : x * x).collect()\n",
    "s\n",
    "# collect를 안쓰면 결과가 나오지 않는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatmap() : 리스트들의 원소를 하나의 리스트로 flatten해서 return하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'spark', 'hi', 'python', 'hi', 'django', 'hi', 'sklearn']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = spark.parallelize(['hi spark', 'hi python', 'hi django', 'hi sklearn'])\n",
    "unique_string = strings.flatMap(lambda x : x.split(' ')).collect()\n",
    "unique_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[8] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = spark.parallelize(list(range(1, 30, 3)))\n",
    "num\n",
    "# RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 10, 16, 22, 28]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = num.filter(lambda x : x % 2 == 0). collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDD\n",
    "key-value 쌍으로 이루어진 RDD\n",
    "- python tuple을 의미한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[24] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair rdd 생성\n",
    "pairRDD = spark.parallelize([(1,3),(1,6),(2,4),(3,3)])\n",
    "# pairRDD = spark.parallelize([(1,3),(2,4),(3,3),(1,6)]) # sortBykey() test용\n",
    "pairRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reduceByKey()\n",
    "- mapValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 9, 2: 4, 3: 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    i:j\n",
    "    for i, j in pairRDD.reduceByKey(lambda x, y : x+y).collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 36, 2: 16, 3: 9}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    i:j\n",
    "    for i, j in pairRDD.mapValues(lambda x : x ** 2).collect()\n",
    "}\n",
    "# 1:36을 보면 6의 제곱이다. key 값이 중복되면 뒤에 나온 value로 적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x1ef6ba11c18>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x1ef6ba11cf8>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x1ef6ba11da0>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 4, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (1, 6), (2, 4), (3, 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.sortByKey().collect()\n",
    "# key 순서대로? 정렬된다.\n",
    "# (1,6)을 뒤로 보낸 후 test하더라도 해당 결과가 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 외부 데이터를 로드해서 RDD 생성하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./data/spark-rdd-name-customers.csv MapPartitionsRDD[27] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerRDD = spark.textFile('./data/spark-rdd-name-customers.csv')\n",
    "customerRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alfreds Futterkiste,Germany'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerRDD.first()\n",
    "# pairRDD 객체임을 확인 가능 (key, value)\n",
    "# first()만 해줘도 결과가 나오므로 이 자체가 Action이라고 보면 됨\n",
    "# 그 근거로 .collect()를 해줄 경우 Error가 나옴\n",
    "# key : Alfreds Futterkiste, value : Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', 'Alfreds Futterkiste'),\n",
       " ('Mexico', 'Ana Trujillo Emparedados y helados'),\n",
       " ('Mexico', 'Antonio Moreno Taqueria'),\n",
       " ('UK', 'Around the Horn'),\n",
       " ('Sweden', 'Berglunds snabbkop'),\n",
       " ('Germany', 'Blauer See Delikatessen'),\n",
       " ('France', 'Blondel pere et fils'),\n",
       " ('Spain', 'Bolido Comidas preparadas'),\n",
       " ('France', \"Bon app'\"),\n",
       " ('Canada', 'Bottom-Dollar Marketse'),\n",
       " ('UK', \"B's Beverages\"),\n",
       " ('Argentina', 'Cactus Comidas para llevar'),\n",
       " ('Mexico', 'Centro comercial Moctezuma'),\n",
       " ('Switzerland', 'Chop-suey Chinese'),\n",
       " ('Brazil', 'Comercio Mineiro'),\n",
       " ('UK', 'Consolidated Holdings'),\n",
       " ('Germany', 'Drachenblut Delikatessend'),\n",
       " ('France', 'Du monde entier'),\n",
       " ('UK', 'Eastern Connection'),\n",
       " ('Austria', 'Ernst Handel'),\n",
       " ('Brazil', 'Familia Arquibaldo'),\n",
       " ('Spain', 'FISSA Fabrica Inter. Salchichas S.A.'),\n",
       " ('France', 'Folies gourmandes'),\n",
       " ('Sweden', 'Folk och fa HB'),\n",
       " ('Germany', 'Frankenversand'),\n",
       " ('France', 'France restauration'),\n",
       " ('Italy', 'Franchi S.p.A.'),\n",
       " ('Portugal', 'Furia Bacalhau e Frutos do Mar'),\n",
       " ('Spain', 'Galeria del gastronomo'),\n",
       " ('Spain', 'Godos Cocina Tipica'),\n",
       " ('Brazil', 'Gourmet Lanchonetes'),\n",
       " ('USA', 'Great Lakes Food Market'),\n",
       " ('Venezuela', 'GROSELLA-Restaurante'),\n",
       " ('Brazil', 'Hanari Carnes'),\n",
       " ('Venezuela', 'HILARIoN-Abastos'),\n",
       " ('USA', 'Hungry Coyote Import Store'),\n",
       " ('Ireland', 'Hungry Owl All-Night Grocers'),\n",
       " ('UK', 'Island Trading'),\n",
       " ('Germany', 'Koniglich Essen'),\n",
       " ('France', \"La corne d'abondance\"),\n",
       " ('France', \"La maison d'Asie\"),\n",
       " ('Canada', 'Laughing Bacchus Wine Cellars'),\n",
       " ('USA', 'Lazy K Kountry Store'),\n",
       " ('Germany', 'Lehmanns Marktstand'),\n",
       " ('USA', \"Let's Stop N Shop\"),\n",
       " ('Venezuela', 'LILA-Supermercado'),\n",
       " ('Venezuela', 'LINO-Delicateses'),\n",
       " ('USA', 'Lonesome Pine Restaurant'),\n",
       " ('Italy', 'Magazzini Alimentari Riuniti'),\n",
       " ('Belgium', 'Maison Dewey'),\n",
       " ('Canada', 'Mere Paillarde'),\n",
       " ('Germany', 'Morgenstern Gesundkost'),\n",
       " ('UK', 'North/South'),\n",
       " ('Argentina', 'Oceano Atlantico Ltda.'),\n",
       " ('USA', 'Old World Delicatessen'),\n",
       " ('Germany', 'Ottilies Kaseladen'),\n",
       " ('France', 'Paris specialites'),\n",
       " ('Mexico', 'Pericles Comidas clasicas'),\n",
       " ('Austria', 'Piccolo und mehr'),\n",
       " ('Portugal', 'Princesa Isabel Vinhoss'),\n",
       " ('Brazil', 'Que Delicia'),\n",
       " ('Brazil', 'Queen Cozinha'),\n",
       " ('Germany', 'QUICK-Stop'),\n",
       " ('Argentina', 'Rancho grande'),\n",
       " ('USA', 'Rattlesnake Canyon Grocery'),\n",
       " ('Italy', 'Reggiani Caseifici'),\n",
       " ('Brazil', 'Ricardo Adocicados'),\n",
       " ('Switzerland', 'Richter Supermarkt'),\n",
       " ('Spain', 'Romero y tomillo'),\n",
       " ('Norway', 'Sante Gourmet'),\n",
       " ('USA', 'Save-a-lot Markets'),\n",
       " ('UK', 'Seven Seas Imports'),\n",
       " ('Denmark', 'Simons bistro'),\n",
       " ('France', 'Specialites du monde'),\n",
       " ('USA', 'Split Rail Beer & Ale'),\n",
       " ('Belgium', 'Supremes delices'),\n",
       " ('USA', 'The Big Cheese'),\n",
       " ('USA', 'The Cracker Box'),\n",
       " ('Germany', 'Toms Spezialitaten'),\n",
       " ('Mexico', 'Tortuga Restaurante'),\n",
       " ('Brazil', 'Tradicao Hipermercados'),\n",
       " ('USA', \"Trail's Head Gourmet Provisioners\"),\n",
       " ('Denmark', 'Vaffeljernet'),\n",
       " ('France', 'Victuailles en stock'),\n",
       " ('France', 'Vins et alcools Chevalier'),\n",
       " ('Germany', 'Die Wandernde Kuh'),\n",
       " ('Finland', 'Wartian Herkku'),\n",
       " ('Brazil', 'Wellington Importadora'),\n",
       " ('USA', 'White Clover Markets'),\n",
       " ('Finland', 'Wilman Kala'),\n",
       " ('Poland', 'Wolski')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map 연산자를 이용해서 ,(comma)로 split하고 tuple로 return하는 구문을 작성하기\n",
    "# cusPairs = customerRDD.map(lambda x : (x.split(',')[0], x.split(',')[1]))\n",
    "# 위의 코드와 다르게 key를 앞으로 뺐다.\n",
    "cusPairs = customerRDD.map(lambda x : (x.split(',')[1], x.split(',')[0]))\n",
    "cusPairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- groupByKey() : 키 값을 리스트 형태로 리턴 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', <pyspark.resultiterable.ResultIterable at 0x1ef6ba042b0>),\n",
       " ('Mexico', <pyspark.resultiterable.ResultIterable at 0x1ef6ba04630>),\n",
       " ('UK', <pyspark.resultiterable.ResultIterable at 0x1ef6ba04828>),\n",
       " ('Sweden', <pyspark.resultiterable.ResultIterable at 0x1ef6ba04b70>),\n",
       " ('France', <pyspark.resultiterable.ResultIterable at 0x1ef6ba04588>),\n",
       " ('Spain', <pyspark.resultiterable.ResultIterable at 0x1ef6ba047b8>),\n",
       " ('Canada', <pyspark.resultiterable.ResultIterable at 0x1ef6ba04ac8>),\n",
       " ('Argentina', <pyspark.resultiterable.ResultIterable at 0x1ef6ba04fd0>),\n",
       " ('Switzerland', <pyspark.resultiterable.ResultIterable at 0x1ef6ba04470>),\n",
       " ('Brazil', <pyspark.resultiterable.ResultIterable at 0x1ef6ba046a0>),\n",
       " ('Austria', <pyspark.resultiterable.ResultIterable at 0x1ef6b9a1748>),\n",
       " ('Italy', <pyspark.resultiterable.ResultIterable at 0x1ef6b9a1da0>),\n",
       " ('Portugal', <pyspark.resultiterable.ResultIterable at 0x1ef6b9a1208>),\n",
       " ('USA', <pyspark.resultiterable.ResultIterable at 0x1ef6b9a1f98>),\n",
       " ('Venezuela', <pyspark.resultiterable.ResultIterable at 0x1ef6b9a1d30>),\n",
       " ('Ireland', <pyspark.resultiterable.ResultIterable at 0x1ef6ba3b0b8>),\n",
       " ('Belgium', <pyspark.resultiterable.ResultIterable at 0x1ef6ba11710>),\n",
       " ('Norway', <pyspark.resultiterable.ResultIterable at 0x1ef6ba119e8>),\n",
       " ('Denmark', <pyspark.resultiterable.ResultIterable at 0x1ef6ba2d2b0>),\n",
       " ('Finland', <pyspark.resultiterable.ResultIterable at 0x1ef6ba2d1d0>),\n",
       " ('Poland', <pyspark.resultiterable.ResultIterable at 0x1ef6ba2d128>)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cusPairs.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92390>),\n",
       " ('Mexico', <pyspark.resultiterable.ResultIterable at 0x1ef6ba923c8>),\n",
       " ('UK', <pyspark.resultiterable.ResultIterable at 0x1ef6ba924e0>),\n",
       " ('Sweden', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92438>),\n",
       " ('France', <pyspark.resultiterable.ResultIterable at 0x1ef6ba925c0>),\n",
       " ('Spain', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92630>),\n",
       " ('Canada', <pyspark.resultiterable.ResultIterable at 0x1ef6ba926a0>),\n",
       " ('Argentina', <pyspark.resultiterable.ResultIterable at 0x1ef6ba926d8>),\n",
       " ('Switzerland', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92710>),\n",
       " ('Brazil', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92780>),\n",
       " ('Austria', <pyspark.resultiterable.ResultIterable at 0x1ef6ba927f0>),\n",
       " ('Italy', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92860>),\n",
       " ('Portugal', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92898>),\n",
       " ('USA', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92908>),\n",
       " ('Venezuela', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92940>),\n",
       " ('Ireland', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92550>),\n",
       " ('Belgium', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92a58>),\n",
       " ('Norway', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92ac8>),\n",
       " ('Denmark', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92b38>),\n",
       " ('Finland', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92ba8>),\n",
       " ('Poland', <pyspark.resultiterable.ResultIterable at 0x1ef6ba92c18>)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UK에 사는 고객이름만 출력하기 (dict 형식으로 만들어서)\n",
    "groupKey = cusPairs.groupByKey().collect()\n",
    "groupKey\n",
    "# Iterable 주소번지를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Around the Horn\n",
      "B's Beverages\n",
      "Consolidated Holdings\n",
      "Eastern Connection\n",
      "Island Trading\n",
      "North/South\n",
      "Seven Seas Imports\n"
     ]
    }
   ],
   "source": [
    "for country, names in groupKey :\n",
    "    if country == 'UK' :\n",
    "        for name in names :\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Around the Horn',\n",
       " \"B's Beverages\",\n",
       " 'Consolidated Holdings',\n",
       " 'Eastern Connection',\n",
       " 'Island Trading',\n",
       " 'North/South',\n",
       " 'Seven Seas Imports']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 동일한 결과를 list 형식으로 출력\n",
    "# 이중 for문\n",
    "customerDict = {\n",
    "    country : [ name for name in names ] for country, names in groupKey\n",
    "}\n",
    "\n",
    "customerDict['UK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Argentina',\n",
       " 'Argentina',\n",
       " 'Argentina',\n",
       " 'Austria',\n",
       " 'Austria',\n",
       " 'Belgium',\n",
       " 'Belgium',\n",
       " 'Brazil',\n",
       " 'Brazil',\n",
       " 'Brazil']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey : key 오름차순으로 정렬하고 상위 10개 추출하기\n",
    "cusPairs.sortByKey().keys().collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 나라별 고객이 몇명인지 count하기\n",
    "mapR = cusPairs.mapValues(lambda x : 1)\n",
    "# mapR.collect()\n",
    "# 각각의 value가 1로 지정되고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', 11),\n",
       " ('Mexico', 5),\n",
       " ('UK', 7),\n",
       " ('Sweden', 2),\n",
       " ('France', 11),\n",
       " ('Spain', 5),\n",
       " ('Canada', 3),\n",
       " ('Argentina', 3),\n",
       " ('Switzerland', 2),\n",
       " ('Brazil', 9),\n",
       " ('Austria', 2),\n",
       " ('Italy', 3),\n",
       " ('Portugal', 2),\n",
       " ('USA', 13),\n",
       " ('Venezuela', 4),\n",
       " ('Ireland', 1),\n",
       " ('Belgium', 2),\n",
       " ('Norway', 1),\n",
       " ('Denmark', 2),\n",
       " ('Finland', 2),\n",
       " ('Poland', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceBykey 활용해서 같은 key 값들을 누적하여 count\n",
    "mapR = cusPairs.mapValues(lambda x : 1).reduceByKey(lambda x, y : x+y)\n",
    "mapR.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Germany': 11,\n",
       " 'Mexico': 5,\n",
       " 'UK': 7,\n",
       " 'Sweden': 2,\n",
       " 'France': 11,\n",
       " 'Spain': 5,\n",
       " 'Canada': 3,\n",
       " 'Argentina': 3,\n",
       " 'Switzerland': 2,\n",
       " 'Brazil': 9,\n",
       " 'Austria': 2,\n",
       " 'Italy': 3,\n",
       " 'Portugal': 2,\n",
       " 'USA': 13,\n",
       " 'Venezuela': 4,\n",
       " 'Ireland': 1,\n",
       " 'Belgium': 2,\n",
       " 'Norway': 1,\n",
       " 'Denmark': 2,\n",
       " 'Finland': 2,\n",
       " 'Poland': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 같은 결과를 for문을 활용하여 출력\n",
    "{\n",
    "    i: j\n",
    "    for i, j in mapR.collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame\n",
    "RDD의 확장된 구조\n",
    "- 행, 열로 이루어진 내장 RDD\n",
    "\n",
    "### 생성\n",
    "- Spark session을 활용한 생성\n",
    "- SQL context의 테이블을 통한 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x1ef6ba87b00>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlCtx = SQLContext(spark)\n",
    "sqlCtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"brand\":\"Ford\", \"models\":{\"name\":\"Fiesta\", \"price\": \"14260\"}}',\n",
       " '{\"brand\":\"Ford\", \"models\":{\"name\": \"Focus\", \"price\": \"18825\"}}',\n",
       " '{\"brand\":\"Ford\", \"models\":{\"name\": \"Mustang\", \"price\": \"26670\"}}',\n",
       " '{\"brand\":\"BMW\", \"models\":{\"name\":\"320\", \"price\": \"40250\"}}',\n",
       " '{\"brand\":\"BMW\", \"models\":{\"name\":\"X3\", \"price\": \"41000\"}}',\n",
       " '{\"brand\":\"BMW\", \"models\":{\"name\":\"X5\", \"price\": \"60700\"}}',\n",
       " '{\"brand\":\"Fiat\", \"models\":{\"name\":\"500\", \"price\": \"16495\"}}']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json 파일\n",
    "# json -> RDD -> DataFrame\n",
    "sample_json = spark.textFile('./data/cars.json')\n",
    "sample_json.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(brand='Ford', models={'name': 'Fiesta', 'price': '14260'}),\n",
       " Row(brand='Ford', models={'name': 'Focus', 'price': '18825'}),\n",
       " Row(brand='Ford', models={'name': 'Mustang', 'price': '26670'}),\n",
       " Row(brand='BMW', models={'name': '320', 'price': '40250'}),\n",
       " Row(brand='BMW', models={'name': 'X3', 'price': '41000'}),\n",
       " Row(brand='BMW', models={'name': 'X5', 'price': '60700'}),\n",
       " Row(brand='Fiat', models={'name': '500', 'price': '16495'})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_df = sqlCtx.createDataFrame( sample_json.map(lambda x : json.loads(x)) )\n",
    "cars_df.collect()\n",
    "# collect 해서 Row가 나오는데 Row는 RDD를 내장하고 있는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- models: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|brand|              models|\n",
      "+-----+--------------------+\n",
      "| Ford|[name -> Fiesta, ...|\n",
      "| Ford|[name -> Focus, p...|\n",
      "| Ford|[name -> Mustang,...|\n",
      "|  BMW|[name -> 320, pri...|\n",
      "|  BMW|[name -> X3, pric...|\n",
      "|  BMW|[name -> X5, pric...|\n",
      "| Fiat|[name -> 500, pri...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|brand|\n",
      "+-----+\n",
      "| Ford|\n",
      "| Ford|\n",
      "| Ford|\n",
      "|  BMW|\n",
      "|  BMW|\n",
      "|  BMW|\n",
      "| Fiat|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame에 대한 연산\n",
    "# select()\n",
    "\n",
    "cars_df.select('brand').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|14260|\n",
      "|18825|\n",
      "|26670|\n",
      "|40250|\n",
      "|41000|\n",
      "|60700|\n",
      "|16495|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.select('models.price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼의 타입 변환\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[brand: string, name: string, price: string]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_price_type = cars_df.select('brand','models.name','models.price')\n",
    "cars_price_type\n",
    "# type이 string임을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "|brand|   name|price|\n",
      "+-----+-------+-----+\n",
      "| Ford| Fiesta|14260|\n",
      "| Ford|  Focus|18825|\n",
      "| Ford|Mustang|26670|\n",
      "|  BMW|    320|40250|\n",
      "|  BMW|     X3|41000|\n",
      "|  BMW|     X5|60700|\n",
      "| Fiat|    500|16495|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IntegerType() 를 활용해서 price를 정수형으로 변환\n",
    "cars_price_type.withColumn('price', cars_price_type['price'].cast(IntegerType()))\n",
    "cars_price_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_price_type.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(brand='Ford', name='Fiesta', price='14260')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_price_type.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(brand='Ford', name='Fiesta', price='14260'),\n",
       " Row(brand='Ford', name='Focus', price='18825'),\n",
       " Row(brand='Ford', name='Mustang', price='26670'),\n",
       " Row(brand='BMW', name='320', price='40250'),\n",
       " Row(brand='BMW', name='X3', price='41000'),\n",
       " Row(brand='BMW', name='X5', price='60700'),\n",
       " Row(brand='Fiat', name='500', price='16495')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비교 연산\n",
    "# collect 해서 Row가 나오는데 Row는 RDD를 내장하고 있는 것\n",
    "cars_price_type.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "|brand|   name|price|\n",
      "+-----+-------+-----+\n",
      "| Ford|Mustang|26670|\n",
      "|  BMW|    320|40250|\n",
      "|  BMW|     X3|41000|\n",
      "|  BMW|     X5|60700|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 가격이 20000 이상인 열을 추출\n",
    "cars_price_type.filter(cars_price_type['price'] > 20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|brand|count|\n",
      "+-----+-----+\n",
      "|  BMW|    3|\n",
      "| Fiat|    1|\n",
      "| Ford|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping\n",
    "# group화 해서 count\n",
    "cars_price_type.groupBy('brand').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
